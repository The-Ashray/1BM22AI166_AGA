{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mT1hvIgz2cf",
        "outputId": "d3aadc09-3328-4186-85c7-f4ffa0b81633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1, Loss: 137.9386\n",
            "Epoch 2, Loss: 125.4968\n",
            "Epoch 3, Loss: 119.6653\n",
            "Epoch 4, Loss: 116.9123\n",
            "Epoch 5, Loss: 115.1583\n",
            "Epoch 6, Loss: 112.3009\n",
            "Epoch 7, Loss: 112.0348\n",
            "Epoch 8, Loss: 110.9090\n",
            "Epoch 9, Loss: 109.3577\n",
            "Epoch 10, Loss: 109.3811\n",
            "Training Complete!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "\n",
        "# Define VAE Model\n",
        "class VAE(Model):\n",
        "    def __init__(self, latent_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            layers.Dense(400, activation='relu'),\n",
        "            layers.Dense(latent_dim * 2)  # mean and log variance\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            layers.Dense(400, activation='relu'),\n",
        "            layers.Dense(784, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def reparameterize(self, z_mean, z_logvar):\n",
        "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
        "        return z_mean + tf.exp(0.5 * z_logvar) * eps\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.encoder(x)\n",
        "        z_mean, z_logvar = tf.split(x, num_or_size_splits=2, axis=1)\n",
        "        z = self.reparameterize(z_mean, z_logvar)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon, z_mean, z_logvar\n",
        "\n",
        "# Loss Function\n",
        "def vae_loss(x, x_recon, z_mean, z_logvar):\n",
        "    recon_loss = binary_crossentropy(x, x_recon) * 784\n",
        "    kl_loss = -0.5 * tf.reduce_sum(1 + z_logvar - tf.square(z_mean) - tf.exp(z_logvar), axis=1)\n",
        "    return tf.reduce_mean(recon_loss + kl_loss)\n",
        "\n",
        "# Load Data\n",
        "(train_images, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(-1, 784).astype('float32') / 255.0\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(128)\n",
        "\n",
        "# Train Model\n",
        "vae = VAE()\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "for epoch in range(10):\n",
        "    for x in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            x_recon, z_mean, z_logvar = vae(x)\n",
        "            loss = vae_loss(x, x_recon, z_mean, z_logvar)\n",
        "        grads = tape.gradient(loss, vae.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "print(\"Training Complete!\")\n"
      ]
    }
  ]
}